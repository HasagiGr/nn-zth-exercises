# nn-zth-exercises
Exercises from Andrej Karpathy nn-zero-to-hero 

Repository with lecture materials: [karpathy/nn-zero-to-hero](https://github.com/karpathy/nn-zero-to-hero)

# Lectures 
## Lecture 1 
[The spelled-out intro to neural networks and backpropagation: building micrograd](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=5040s)


Exercises:
 - [&check;] Section 1. Derivatives.
 - [&check;] Section 2. Support for softmax.

## Lecture 2
[The spelled-out intro to language modeling: building makemore](https://www.youtube.com/watch?v=PaCmpygFfXo&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=2)


Exercises:
- [&check;] Exercise 1. Trigram model.
- [&check;] Exercise 2. Split into train/dev/test sets.
- [&check;] Exercise 3. Regularization.
- [&check;] Exercise 4. Get rid of one-hot vectors.
- [&check;] Exercise 5. Use cross entropy instead.

## Lecture 3
[Building makemore Part 2: MLP](https://www.youtube.com/watch?v=TCH_1BHY58I&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=3)


Exercises:
- [&check;] Exercise 1. Tune hyperparameters.
- [&check;] Exercise 2. Proper initialization.
- [&check;] Exercise 3. Implement any feature from paper Bengio(2003) [A NEURAL PROBABILISTIC LANGUAGE MODEL](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

## Lecture 4
[Building makemore Part 3: Activations & Gradients, BatchNorm](https://www.youtube.com/watch?v=P6sfmUTpUmc&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=4)

Exercises:
- [&check;] Exercise 1. Initializing weights and biases to zero.
- [&check;] Exercise 2. BatchNorm usage.

## Lecture 5
[Building makemore Part 4: Becoming a Backprop Ninja](https://www.youtube.com/watch?v=q8SA3rM6ckI&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=5)


Exercises:
- [ ] Exercise 1. Backprop through the whole thing manually.
- [ ] Exercise 2. Backprop through cross_entropy but all in one go.
- [ ] Exercise 3. Backprop through batchnorm but all in one go.
- [ ] Exercise 4. Putting all together.

## Lecture 6
[Building makemore Part 5: Building a WaveNet](https://www.youtube.com/watch?v=t3YJ5hKiMQ0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=6)


No exercises.

## Lecture 7
[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7)


Exercises:
- [ ] Exercise 1. Combine the `Head` and `MultiHeadAttention` into one class.
- [ ] Exercise 2. Train the GPT on your own dataset.
- [ ] Exercise 3. Pretraining on large dataset.
- [ ] Exercise 4. Implement one additional feature from any transformer paper.
